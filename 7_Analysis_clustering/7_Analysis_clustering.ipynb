{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons Licence\" style=\"width=50\" src=\"https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png\" title='This work is licensed under a Creative Commons Attribution 4.0 International License.' align=\"right\"/></a>\n",
    "\n",
    "**Authors**: Dr Matteo Degiacomi (matteo.t.degiacomi@durham.ac.uk) and Dr Antonia Mey (antonia.mey@ed.ac.uk)\n",
    "\n",
    "Content is partially adapted from the [Software Carpentries Machine learning lesson](https://carpentries-incubator.github.io/machine-learning-novice-sklearn/index.html) and [here](https://github.com/christianversloot/machine-learning-articles/blob/main/performing-dbscan-clustering-with-python-and-scikit-learn.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "- Identify clusters in data using *k-means* clustering, *DBSCAN* and *spectral clustering*. \n",
    "- See the limitations of k-means when clusters overlap.\n",
    "- Use other clustering methods to overcome the limitations of k-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jupyter cheat sheet**\n",
    "- to run the currently highlighted cell, hold <kbd>&#x21E7; Shift</kbd> and press <kbd>&#x23ce; Enter</kbd>;\n",
    "- to get help for a specific function, place the cursor within the function's brackets, hold <kbd>&#x21E7; Shift</kbd>, and press <kbd>&#x21E5; Tab</kbd>;\n",
    "\n",
    "<div class=\"alert alert-info\"><b> Remember: variables persist between cells</b> \n",
    "    \n",
    "Be aware that it is the order of execution of cells that is important in a Jupyter notebook, not the <em>order</em> in which they appear. Python will remember <em>all</em> the code that was run previously, including any variables you have defined, irrespective of the order in the notebook. Therefore if you define variables lower down the notebook and then (re)run cells further up, those defined further down will still be present. </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "[0.   Google Colab Setup](#setup)      \n",
    "[1.   Introduction](#intro)   \n",
    "[2.   K-means clustering](#kmeans)   \n",
    "[3.   DBSCAN: a density-based clustering scheme](#dbscan)   \n",
    "[4.   Dihedral space of alanine dipeptide](#dihedrals)    \n",
    "[5.   Conclusion](#conclusion)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Google Colab setup\n",
    "<a id='setup'></a>\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Attention:</b> Please only run the following cells if you are using Colab! These cells install necessary packages and download data.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!if [ -n \"$COLAB_RELEASE_TAG\" ]; then pip install condacolab; fi\n",
    "import condacolab\n",
    "condacolab.install()\n",
    "\n",
    "import condacolab\n",
    "!mamba install -c conda-forge mdanalysis mdanalysistests mdanalysisdata nglview scikit-learn ipywidgets=7.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable third party jupyter widgets\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!if [ -n \"$COLAB_RELEASE_TAG\" ]; then git clone https://github.com/CCPBioSim/CCP5_Simulation_of_BioMolecules; fi\n",
    "import os\n",
    "os.chdir(f\"CCP5_Simulation_of_BioMolecules{os.sep}7_Analysis_clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "nbgrader": {},
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## 1. Introduction\n",
    "<a id='intro'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is the grouping of data points which are similar to each other. It can be a powerful technique for identifying patterns in data. Clustering is known as an *unsupervised learning* technique, whereby no example data needs to be provided for this task to be carried out. Application of clustering are:\n",
    "- Looking for trends in data\n",
    "- Data compression, all data clustering around a point can be reduced to just that point (e.g., reducing colour depth of an image)\n",
    "- Pattern recognition.\n",
    "\n",
    "In this tutorial, we will look at common clustering techniques, k-Means, DBASCAN, and Spectral Clustering, and we will apply them first to toy data, and then on the molecular dynamics simulation on alanine dipeptide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "nbgrader": {},
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## 2. K-means clustering\n",
    "<a id='kmeans'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means is an algorithm that aggregates data into clusters by manipulating the position of cluster centres (centroids). Its goal is to search for the ideal position of centrods such that the average distance between each data point and its closest centroid is minimized. The algorithm needs to be told **how many clusters** to define, but a common technique is to try different numbers of clusters and combine it with other tests to decide on the number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "nbgrader": {},
    "new_sheet": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "To perform a [K-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) clustering with Scikit-learn, we first need to import the `sklearn.cluster` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster as skl_cluster\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Creating the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will use scikit-learn’s built in random data blob generator instead of using an external dataset. For this we will also need the `sklearn.datasets.samples_generator` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as skl_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s create some random blobs using the `make_blobs` function. The `n_samples` argument sets how many points we want to use in all of our blobs. `cluster_std` sets the standard deviation of the points, the smaller this value the closer together they will be. `centers` sets the desired number of clusters. `random_state` is the initial state of the random number generator, by specifying its value we will get the same results every time we run the program. If we do not specify a random state, then we will get different points every time we run. This function returns two things: an array of data points and a list of which cluster each point belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "nbgrader": {},
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "data, cluster_id = skl_datasets.make_blobs(n_samples=400, cluster_std=0.75, centers=4, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Plotting and clustering the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now created the data, let's have a look at what `make_blobs` gave us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(data[:, 0], data[:, 1], s=5, linewidth=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's identify the clusters using K-means. First, we need to initialise the `KMeans` module. As parameters, we tell it how many clusters to look for (`n_clusters=4`), and how many times to repeat the clustering (`n_init=10`, the attempt yielding the smallest average distance of datapoints from the closest centroid will be returned). The latter parameter is especially useful for sparse data, where the performance of `KMeans` is more affected by poor initial conditions. Next, we define what data needs to be clustered via the `fit` function. Finally, we run the predict function to find the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialise Kmeans\n",
    "Kmean = skl_cluster.KMeans(n_clusters=4, n_init=10)\n",
    "# 2. Fit the data\n",
    "Kmean.fit(data)\n",
    "# 3. Predict the clusters\n",
    "clusters = Kmean.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can now be plotted to show all the points we randomly generated. To clarify to which cluster each data point has been assigned to, we can define their colour (the `c` parameter) using the <code>clusters</code> list of integers returned by the predict function. Data points belonging to the same cluster will be associated to the same integer number in this list. The K-means algorithm also lets us know where the centre of each cluster is located. Cluster centres are stored as a list called `cluster_centers_` inside the `Kmean` object. Let’s go ahead and plot the points from the clusters, colouring them by the output from the K-means algorithm, and also plotting the centres of each cluster as a red triangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:, 0], data[:, 1], s=5, linewidth=0, c=clusters)\n",
    "for cluster_x, cluster_y in Kmean.cluster_centers_:\n",
    "    plt.scatter(cluster_x, cluster_y, s=100, c='r', marker='^')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "nbgrader": {},
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Working in multiple dimensions.</b>\n",
    "Data manipulated in a machine learning context is ofter high-dimensional. Although the example presented her shows the clustering of 2-dimensional data, the k-means algorithm can operate in an arbitrarily high number of dimensions. It is however difficult to show this visually once we get beyond 3 dimensions.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Task section\n",
    "Please work in small groups for the next 15 minutes on the tasks below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 1.</b> Discuss: What are the limitations and advantages of K-Means? </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: Suggested limitations and advantages</mark> </summary>\n",
    "\n",
    "Limitations:\n",
    "- Requires number of clusters to be known in advance,\n",
    "- Struggles when clusters have irregular shapes,\n",
    "- Will always produce an answer finding the required number of clusters even if the data features a different number of clusters, or no cluster at all,\n",
    "- Requires linear cluster boundaries.\n",
    "\n",
    "Advantages:\n",
    "- Simple algorithm, fast to compute. A good choice as the first thing to try when attempting to cluster data,\n",
    "- Suitable for large datasets due to its low memory and computing requirements.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 2.</b> K-means with overlapping clusters. Adjust the program below to increase the standard deviation of the blobs (the <code>cluster_std</code> parameter to <code>make_blobs</code>) and increase the number of samples (<code>n_samples</code>) to 4000. Visually, you should start to see the blobs start overlapping. Do the clusters that are identified make sense?\n",
    "    \n",
    "```Python\n",
    "data, cluster_id = skl_datasets.make_blobs(n_samples=400, cluster_std=0.75, centers=4, random_state=1)\n",
    "\n",
    "Kmean = skl_cluster.KMeans(n_clusters=4)\n",
    "Kmean.fit(data)\n",
    "clusters = Kmean.predict(data)\n",
    "\n",
    "plt.scatter(data[:, 0], data[:, 1], s=5, linewidth=0, c=clusters)\n",
    "for cluster_x, cluster_y in Kmean.cluster_centers_:\n",
    "    plt.scatter(cluster_x, cluster_y, s=100, c='r', marker='^')\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your solution here! ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution</mark> </summary>\n",
    "\n",
    "```Python\n",
    "data, cluster_id = skl_datasets.make_blobs(n_samples=4000, cluster_std=3.0, centers=4, random_state=1)\n",
    "\n",
    "Kmean = skl_cluster.KMeans(n_clusters=4)\n",
    "Kmean.fit(data)\n",
    "clusters = Kmean.predict(data)\n",
    "\n",
    "plt.scatter(data[:, 0], data[:, 1], s=5, linewidth=0, c=clusters)\n",
    "for cluster_x, cluster_y in Kmean.cluster_centers_:\n",
    "    plt.scatter(cluster_x, cluster_y, s=100, c='r', marker='^')\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 3.</b> How many clusters should we look for?\n",
    "As K-Means requires us to specify the number of clusters to expect a common strategy to get around this is to vary the number of clusters we are looking for. Modify the program to loop through searching for between 2 and 10 clusters. Which (if any) of the results look more sensible? What criteria might you use to select the best one?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your solution here! ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution</mark> </summary>\n",
    "\n",
    "```Python\n",
    "for cluster_count in range(2,11):\n",
    "    Kmean = skl_cluster.KMeans(n_clusters=cluster_count)\n",
    "    Kmean.fit(data)\n",
    "    clusters = Kmean.predict(data)\n",
    "    plt.scatter(data[:, 0], data[:, 1], s=5, linewidth=0,c=clusters)\n",
    "    for cluster_x, cluster_y in Kmean.cluster_centers_:\n",
    "        plt.scatter(cluster_x, cluster_y, s=100, c='r', marker='^')\n",
    "        # give the graph a title with the number of clusters\n",
    "        plt.title(str(cluster_count)+\" Clusters\")\n",
    "    plt.show()\n",
    "```\n",
    "None of these look very sensible clusterings because all the points really form one large cluster. We might look at a measure of similarity of the cluster to test if its really multiple clusters. A simple standard deviation or interquartile range might be a good starting point.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DBSCAN: a density based clustering scheme\n",
    "<a id='dbscan'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Generating and plotting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, data might be arranged according to non-spherical clusters. An (extreme) example of such a situation, is that of data points organized in two concentric rings. Scikit-learn has an inbuilt version of generating such data, similar to the `make_blobs` function, it is called `make_circles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as skl_data\n",
    "circles, circles_clusters = skl_data.make_circles(n_samples=400, noise=.01, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data to see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(circles[:, 0], circles[:, 1], s=5, linewidth=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Clustering the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we use k-means to cluster this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kmean = skl_cluster.KMeans(n_clusters=2)\n",
    "Kmean.fit(circles)\n",
    "clusters = Kmean.predict(circles)\n",
    "\n",
    "plt.scatter(circles[:, 0], circles[:, 1], s=5, linewidth=0, c=clusters)\n",
    "for cluster_x, cluster_y in Kmean.cluster_centers_:\n",
    "    plt.scatter(cluster_x, cluster_y, s=100, c='r', marker='^')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh dear... this is not ideal! Scikit-learn offers other clustering algorithm. Let's try [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) (Density-Based Spatial Clustering of Applications with Noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = skl_cluster.DBSCAN(eps=0.1)\n",
    "db.fit(circles)\n",
    "clusters = db.labels_.astype(int)\n",
    "no_clusters = len(np.unique(clusters) )\n",
    "no_noise = np.sum(np.array(clusters) == -1, axis=0)\n",
    "\n",
    "print(f'Estimated no. of clusters: {no_clusters}')\n",
    "print(f'Estimated no. of noise points: {no_noise}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = list(map(lambda x: '#3b4cc0' if x == 1 else '#b40426', clusters))\n",
    "plt.scatter(circles[:,0], circles[:,1], c=colors, marker=\"o\", picker=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN produced a much better clustering. This algorithm iteratively aggregates points to clusters according to a distance criterion `eps`. A point will be added to a cluster if it is at a distance smaller than `eps` from any point already part of it. As a result, the clusters discovered via DBSCAN can have any shape, and their number does not need to be known in advance. An interesting feature of DBSCAN is that it can be also used to identify data outliers (i.e. points that are not assigned to any cluster)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Task section\n",
    "Please work in small groups for the next 15 minutes on the tasks below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 4.</b> What happens when you change the epsilon value of DBSCAN? </div>\n",
    "For the two rings it is quite obvious that you want two clusters to describe this. DBSCAN can be used to identify different numbers of clusters. Play around with the epsilon value to figure out at which point DBSCAN may fail in classifying the two rings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your solution here! ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution</mark> </summary>\n",
    "    \n",
    "```Python\n",
    "epsilon = np.linspace(0.05,0.3,10)\n",
    "for eps in epsilon:\n",
    "    db = skl_cluster.DBSCAN(eps=eps)\n",
    "    db.fit(circles)\n",
    "    clusters = db.labels_.astype(int)\n",
    "    no_clusters = len(np.unique(clusters) )\n",
    "    no_noise = np.sum(np.array(clusters) == -1, axis=0)\n",
    "    print(f'=========== Current epsilon value is: {eps:.2f} ==========')\n",
    "    print(f'Estimated no. of clusters: {no_clusters}')\n",
    "    print(f'Estimated no. of noise points: {no_noise}')\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 5.</b> Can spectral clustering also recognise the two rings? </div>\n",
    "\n",
    "You have seen that k-means is bad at distinguishing the two rings. DBSCAN did manage to do this with a well chosen epsilon value. Another method called Spectral Clustering can also be used to cluster data. Look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html) and see if you can get the spectral clustering method to work for the ring dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your solution here! ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution</mark> </summary>\n",
    "\n",
    "The code for calculating the SpectralClustering is very similar to the k-means clustering, instead of using the `sklearn.cluster.KMeans` class we use the `sklearn.cluster.SpectralClustering` class. The `SpectralClustering` class combines the `fit` and `predict` functions into a single function called `fit_predict`. This is how the solution for clustering the dataset using Spectral clustering looks like:\n",
    "\n",
    "```Python\n",
    "import sklearn.cluster as skl_cluster\n",
    "import sklearn.datasets as skl_data\n",
    "\n",
    "circles, circles_clusters = skl_data.make_circles(n_samples=1000, noise=.01, random_state=0)\n",
    "\n",
    "# cluster with spectral clustering\n",
    "model = skl_cluster.SpectralClustering(n_clusters=2, affinity='nearest_neighbors', assign_labels='kmeans')\n",
    "labels = model.fit_predict(circles)\n",
    "#colors = list(map(lambda x: '#3b7cc0' if x == 1 else '#111426', labels))\n",
    "plt.scatter(circles[:, 0], circles[:, 1], s=15, linewidth=0, c=labels)\n",
    "plt.show()\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 6 (Optional).</b> Performance of clustering algorithms. \n",
    "\n",
    "Combine the code from the k-means clustering we have shown above for the two circles and your solution to the last task to using the spectral clustering. Using the `time` module, time how long both clustering algorithms take to run. To get the start time, get something like `start_time = time.time()`. At the end of the clustering snipptet part for each clustering method get `finish_time` in the same way as `start_time`. Subtracting each of the time components you can figure out how long the spectral and k-means clustering takes. Compare how long both parts take to run generating 4,000 samples and testing them for between 2 and 10 clusters. How much did your run times differ? How much do they differ if you increase the number of samples to 8,000? How long do you think it would take to compute 800,000 samples (estimate this, it might take a while to run for real)?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution</mark> </summary>\n",
    "\n",
    "Runtime is typically ~4 seconds (depending on the computer used)\n",
    "\n",
    "```Python\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.cluster as skl_cluster\n",
    "from sklearn.datasets import make_blobs \n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "data, cluster_id = make_blobs(n_samples=4000, cluster_std=3,\n",
    "                                       centers=4, random_state=1)\n",
    "\n",
    "for cluster_count in range(2,11):\n",
    "    Kmean = skl_cluster.KMeans(n_clusters=cluster_count)\n",
    "    Kmean.fit(data)\n",
    "    clusters = Kmean.predict(data)\n",
    "\n",
    "    plt.scatter(data[:, 0], data[:, 1], s=15, linewidth=0, c=clusters)\n",
    "    plt.title(str(cluster_count)+\" Clusters\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Elapsed time = \", end_time-start_time, \"seconds\")\n",
    "```\n",
    "<br>\n",
    "Spectral version, runtime around 9 seconds (your computer might be faster/slower)\n",
    "<br>\n",
    "    \n",
    "```Python\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.cluster as skl_cluster\n",
    "from sklearn.datasets import make_blobs \n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "data, cluster_id = make_blobs(n_samples=4000, cluster_std=3,\n",
    "                                       centers=4, random_state=1)\n",
    "\n",
    "for cluster_count in range(2,11):\n",
    "    model = skl_cluster.SpectralClustering(n_clusters=cluster_count,\n",
    "                                       affinity='nearest_neighbors',\n",
    "                                       assign_labels='kmeans')\n",
    "    labels = model.fit_predict(data)\n",
    "    \n",
    "    plt.scatter(data[:, 0], data[:, 1], s=15, linewidth=0, c=labels)\n",
    "    plt.title(str(cluster_count)+\" Clusters\")\n",
    "plt.show()\n",
    "end_time = time.time()\n",
    "print(\"Elapsed time = \", end_time-start_time, \"seconds\")\n",
    "    \n",
    "```\n",
    "    \n",
    "When the number of points increases to 8000 the runtimes are 24 seconds for the spectral version and 5.6 seconds for kmeans. The runtime numbers will differ depending on the speed of your computer, but the relative different should be similar. For 4000 points kmeans took 4 seconds, spectral 9 seconds, 2.25 fold difference. For 8000 points kmeans took 5.6 seconds, spectral took 24 seconds. 4.28 fold difference. Kmeans 1.4 times slower for double the data, spectral 2.6 times slower. The realative difference is diverging. Its double by doubling the amount of data. If we use 100 times more data we might expect a 100 fold divergence in execution times. Kmeans might take a few minutes, spectral will take hours.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dihedral space of alanine dipeptide\n",
    "<a id='dihedrals'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Loading and plotting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alanine dipeptide (ADP) is a commonly used toy model in molecular simulations. Let's start by importing some useful packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nglview as nv\n",
    "import warnings\n",
    "import os\n",
    "import MDAnalysis as mda\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load ADP into `nglview` to look at the molecule in three dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adp = mda.Universe(os.path.join('..', 'data', 'alanine-dipeptide-nowater.pdb' ))\n",
    "w = nv.show_mdanalysis(adp)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load information on the backbone dihedral angles $\\phi$ and $\\psi$ of three 250 ns-long simulations of alanine dipeptide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.load(os.path.join('..', 'data', 'alanine-dipeptide-3x250ns-backbone-dihedrals.npz'), mmap_mode='r')\n",
    "dihedral = []\n",
    "for k in temp.files:\n",
    "    dihedral.append(temp[k])\n",
    "dihedral_data = np.concatenate(dihedral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dihedral_data` contains the $\\phi$ and $\\psi$ dihedral angles. Let's take a look at their distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style('ticks')\n",
    "\n",
    "plt.plot(dihedral_data[:,0], dihedral_data[:,1], lw=0, marker='o', ms=0.5)\n",
    "plt.xlabel(r'$\\phi$ in [rad]')\n",
    "plt.ylabel(r'$\\psi$ in [rad]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Task section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 7.</b> What would be a better way to represent this data than a scatter plot? </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution</mark> </summary>\n",
    "The scatter plot we have just produced, called the Ramachandran plot, is often visualised as a probability density.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 8.</b> Try using k-means clustering for the dataset. What do you think is the right number of cluster centers? Can you spot a problem with the clustering of this dataset?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your solution here! ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution</mark> </summary>\n",
    "\n",
    "```Python\n",
    "# cluster the data\n",
    "Kmean = skl_cluster.KMeans(n_clusters=100)\n",
    "Kmean.fit(dihedral_data)\n",
    "clusters = Kmean.predict(dihedral_data)\n",
    "    \n",
    "# plot the ata\n",
    "plt.scatter(dihedral_data[:, 0], dihedral_data[:, 1], s=5, linewidth=0)\n",
    "for cluster_x, cluster_y in Kmean.cluster_centers_:\n",
    "    plt.scatter(cluster_x, cluster_y, s=100, c='r', marker='^')\n",
    "plt.xlabel(r'$\\phi$ in [rad]')\n",
    "plt.ylabel(r'$\\psi$ in [rad]')\n",
    "```\n",
    "<br>\n",
    "It is hard to tell the exact number of clusters, but 2-4 seems like a suitable answer. A problem in this data, is that dihedral angles are periodic.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 9.</b>  Try using DBSCAN to cluster this dataset.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your solution here! ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution</mark> </summary>\n",
    "\n",
    "```Python\n",
    "# cluster the data\n",
    "db = skl_cluster.DBSCAN(eps=0.2)\n",
    "db.fit(dihedral_data[::10])\n",
    "clusters = db.labels_.astype(int)\n",
    "no_clusters = len(np.unique(clusters) )\n",
    "no_noise = np.sum(np.array(clusters) == -1, axis=0)\n",
    "\n",
    "print(f'Estimated no. of clusters: {no_clusters}')\n",
    "print(f'Estimated no. of noise points: {no_noise}')\n",
    "    \n",
    "# plot the data\n",
    "plt.scatter(dihedral_data[::10,0], dihedral_data[::10,1], c=clusters, marker=\"o\", picker=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "<a id='conclusion'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Key points:</b>   \n",
    "\n",
    "- Clustering is a form of unsupervised learning\n",
    "- Unsupervised learning algorithms do not need training   \n",
    "- K-means is a popular clustering algorithm\n",
    "- K-means struggles where one cluster exists within another, such as concentric circles\n",
    "- DBSCAN and Spectral clustering are two other technique which can overcome some of the limitations of k-means \n",
    "- Spectral clustering is much slower than k-means\n",
    "- Different clustering methods can also be applied to real world data such as alanine-dipeptide\n",
    "- As well as providing machine learning algorithms scikit learn also has functions to make example data. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
