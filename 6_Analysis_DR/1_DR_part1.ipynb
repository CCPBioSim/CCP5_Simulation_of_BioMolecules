{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction, part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons Licence\" style=\"width=50\" src=\"https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png\" title='This work is licensed under a Creative Commons Attribution 4.0 International License.' align=\"right\"/></a>\n",
    "\n",
    "**Authors**: Dr Matteo Degiacomi (matteo.t.degiacomi@durham.ac.uk) and Dr Antonia Mey (antonia.mey@ed.ac.uk)\n",
    "\n",
    "Content is partially adapted from the [Software Carpentries Machine learning lesson](https://carpentries-incubator.github.io/machine-learning-novice-sklearn/index.html) and material from the [pyEMMA tutorial](http://www.emma-project.org/latest/tutorials/notebooks/02-dimension-reduction-and-discretization.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "How can we perform unsupervised learning with dimensionality reduction techniques such as Principal Component Analysis (PCA) and time-correlated independent component analysis (tICA)?\n",
    "\n",
    "**Objectives:**\n",
    "- Remember that most data is inherently multidimensional\n",
    "- Understand that reducing the number of dimensions can simplify modelling and allow classifications to be performed\n",
    "- Use PCA as a popular technique for dimensionality reduction\n",
    "- Use tICA, another popular dimensionality reduction technique that takes time correlations into account\n",
    "- Compare how PCA and tICA perform on a 2-D toy example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jupyter cheat sheet**:\n",
    "- to run the currently highlighted cell, hold <kbd>&#x21E7; Shift</kbd> and press <kbd>&#x23ce; Enter</kbd>;\n",
    "- to get help for a specific function, place the cursor within the function's brackets, hold <kbd>&#x21E7; Shift</kbd>, and press <kbd>&#x21E5; Tab</kbd>;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab setup\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Attention:</b> Please only run the following cell if you are using Colab! This cell will download data required for Section 6 of this tutorial.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!if [ -n \"$COLAB_RELEASE_TAG\" ]; then git clone https://github.com/CCPBioSim/CCP5_Simulation_of_BioMolecules; fi\n",
    "import os\n",
    "os.chdir(f\"CCP5_Simulation_of_BioMolecules{os.sep}6_Analysis_DR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "nbgrader": {},
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Scientific data, such as that extracted from molecular dynamics simulations, can be high-dimensional and noisy. Dimensionality reduction is the process identifying and highlighting information and correlations within the data. There are multiple reasons why you might want to do a dimensionality reduction.\n",
    "\n",
    "- You might want to know what are the dominant features in your system (larger scale variations in data).\n",
    "- You want a way to visualise your high dimensional data. \n",
    "- You want to analyse your data, but it it too high-dimensional.\n",
    "\n",
    "The algorithms designed to carry out this task are an example of machine learning. In this tutorial we will look at Principal Components Analysis (PCA), time-lagged independent component analysis (tICA), and t-tested Stocastic Neighbour Embedding (t-SNE). In a machine learning context, each dimension in data is called a **feature**, which together form a **feature space**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 1:</b> Can you think of examples of features that you would find in molecular simulations?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "\n",
    "Examples are:\n",
    "- C-alpha positions\n",
    "- angles\n",
    "- dihedrals\n",
    "- RMSD\n",
    "- density\n",
    "- surface area\n",
    "- ...\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Principal Components Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) is an orthogonal linear transformation that transforms high dimensional data (or feature vectors) into a new coordinate system. In this coordinate system the first coordinate (first *eigenvector*) corresponds to the scalar projection of a linear combination of some data such that this coordinate has the largest variance. The second largest variance in the data can be found in the second coordinate and so on. Let's start by importing some packages required for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create some model data to analyse. To this end, we will exploit the Müller-Brown potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def muller_potential(x, y):\n",
    "    \"\"\"Muller potential\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : {float, np.ndarray}\n",
    "        X coordinate. Can be either a single number or an array. If you supply\n",
    "        an array, x and y need to be the same shape.\n",
    "    y : {float, np.ndarray}\n",
    "        Y coordinate. Can be either a single number or an array. If you supply\n",
    "        an array, x and y need to be the same shape.\n",
    "    Returns\n",
    "    -------\n",
    "    potential : {float, np.ndarray}\n",
    "        Potential energy. Will be the same shape as the inputs, x and y.\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    Code adapted from https://cims.nyu.edu/~eve2/ztsMueller.m\n",
    "    \"\"\"\n",
    "    \n",
    "    aa = [-1, -1, -6.5, 0.7]\n",
    "    bb = [0, 0, 11, 0.6]\n",
    "    cc = [-10, -10, -6.5, 0.7]\n",
    "    AA = [-200, -100, -170, 15]\n",
    "    XX = [1, 0, -0.5, -1]\n",
    "    YY = [0, 0.5, 1.5, 1]\n",
    "    \n",
    "    value = 0\n",
    "    for j in range(0, 4):\n",
    "        value += AA[j] * np.exp(aa[j] * (x - XX[j])**2 + \\\n",
    "            bb[j] * (x - XX[j]) * (y - YY[j]) + cc[j] * (y - YY[j])**2)\n",
    "    return value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now evaluate the potential on a 2-D grid, and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dims = (500, 500)\n",
    "x = np.linspace(-1.5, 1, dims[0])\n",
    "y = np.linspace(-0.4, 1.8, dims[1])\n",
    "X, Y = np.meshgrid(x, y)\n",
    "potential = muller_potential(X, Y)\n",
    "\n",
    "levels = np.linspace(np.min(potential), np.max(potential), 50)\n",
    "plt.contour(X, Y, potential.clip(max=200), 40);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's convert our potential into a probability distribution, and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "Z = np.sum(np.exp(-1/25*potential)) #partition function\n",
    "P = np.exp(-1/25*potential)/Z\n",
    "\n",
    "plt.contour(X, Y, P, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to generate some data! We will extract 10000 samples according to the probability distribution we have just created. To this end, we will use <code>np.random.choice</code>, which enables us to generate random samples according to a given probability. Since this method works only in 1-D, we will first flatten the array, then generate the samples, and finally bring them back to 2-D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "flat = np.ravel(P)\n",
    "sample_index = np.random.choice(a=flat.size, p=flat, size=10000)\n",
    "samples = np.unravel_index(sample_index, P.shape)\n",
    "data = np.array([x[samples[1]], y[samples[0]]]).T\n",
    "\n",
    "plt.scatter(data[:, 0], data[:, 1], c=\"r\", alpha=0.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA can tell us in which features (here the x coordinate is one feature and the y coordinate is the second feature), or rather which linear combination, carries the most variance. Let's do a PCA of the samples we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PCA has identified the two eigenvectors (principal components) of our dataset. Here they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each component represents a percentage of the total variance of the system. Here is how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It is clear that the first component represents majority of the variance in the data. We have identified the two eigenvectors of this dataset. We can now plot them along with the data. To make arrows visible, we will scale their length by the explained variance of each eigenvector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(data[:,0], data[:,1], c=\"r\", alpha=0.05)\n",
    "\n",
    "# plot arrows representing the two components\n",
    "e1_x = pca.components_[0, 0]*pca.explained_variance_ratio_[0]\n",
    "e1_y = pca.components_[0, 1]*pca.explained_variance_ratio_[0]\n",
    "e2_x = pca.components_[1, 0]*pca.explained_variance_ratio_[1]\n",
    "e2_y = pca.components_[1, 1]*pca.explained_variance_ratio_[1]\n",
    "ax.arrow(np.mean(x), np.mean(y), e1_x/pca.explained_variance_ratio_[0], e1_y/pca.explained_variance_ratio_[0], head_width=0.1, head_length=0.1, fc='k', ec='k')\n",
    "ax.arrow(np.mean(x), np.mean(y), e2_x/pca.explained_variance_ratio_[0], e2_y/pca.explained_variance_ratio_[0], head_width=0.1, head_length=0.1, fc='k', ec='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can project the data on the new reference system defined by the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data_projected = data.dot(pca.components_) + pca.mean_\n",
    "print(data_projected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 2: </b> Can you use the PCA results to generate an 1-D approximate for the Müller-Brown potential?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution</mark> </summary>\n",
    "\n",
    "The first principal component represents most of the variance, so we can observe the distribution of data only along this components. This is a way of using PCA as a way of filtering noise, and highlight dominant structures in your data.\n",
    "    \n",
    "```Python\n",
    "plt.hist(data_projected[:,0], bins=50, color=\"r\");\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. time-lagged independent component analysis (tICA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will study tICA with a toy dataset. Let's start by loading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "file = f'..{os.sep}data{os.sep}hmm-doublewell-2d-100k.npz'\n",
    "with np.load(file) as fh:\n",
    "    data = fh['trajectory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Visualising the dataset\n",
    "We can see this is a trajectory with 100000 time datapoints and 2 features. Let's examine this dataset a bit more, plotting the time evolution of each feature in a separate plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axes[0].plot(data[:300, 0], alpha=0.6)\n",
    "axes[1].plot(data[:300, 1], alpha=0.6)\n",
    "axes[1].set_xlabel('$time$')\n",
    "axes[0].set_xlabel('$time$')\n",
    "axes[0].set_ylabel('$x$')\n",
    "axes[1].set_ylabel('$y$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 6: </b> Examine the data to get a better feel for it. What is the extent of the data set in x and y? Can you plot a histogram of the data? What information does the trajectory tell us the histogram obscures? </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "\n",
    "The minimum and maximum of the data is given by:\n",
    "\n",
    "```Python\n",
    "print('x_min is:',np.min(data[:,0]), 'x_max is:',np.max(data[:,0]), '\\ny_min is:', np.min(data[:,1]), 'y_max is:', np.max(data[:,1]))\n",
    "\n",
    "```\n",
    "\n",
    "An example of how to plot a histogram of the data looks like this:\n",
    "    \n",
    "```Python\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "counts,ybins,xbins = np.histogram2d(data[:,0],data[:,1],bins=250);\n",
    "plt.contour(counts,extent=[xbins.min(),xbins.max(),ybins.min(),ybins.max()])\n",
    "```\n",
    "There is no slow transitions in the x coordinate, but there are in the y coordinate.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. tICA analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tICA is a common dimensionality reduction technique for molecular dynamics trajectories. Unfortunately scikit-learn does not feature an implementation of this method, which is why other packages are normally used. Here, we provide a convenient helper module, called <code>tICA</code>, based on the implementation from [MSM-Builder](http://msmbuilder.org/3.8.0/). The module has been adapted so that it can be used as stand-alone, and is written to mimic the syntax of dimensionality reduction techniques available in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from tica.tica import tICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's carry out a tICA analysis of the data we have previously loaded. A small difference from the syntax in scikit-learn: the parameter of the <code>fit</code> method data must be in square brackets, since the method can accept a list of trajectory data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tic = tICA()\n",
    "tic.fit([data]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions identified by the tICA analysis can be accessed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(tic.eigenvectors_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for the next excercise! Before getting to it though, execute the cell below, you will need it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def draw_arrow(origin, v, color):\n",
    "    ax.arrow(origin[0], origin[1], v[0], v[1], color=color, width=0.02, linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 7: </b> Now use PCA on the same dataset and compare the two components by creating a scatter plot and drawing the vectors representing the PCA composition and tICA composition. Make use of the handy helper function <code>draw_arrow</code> for the vectors to draw arrows on the scatter plot from above. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Your solution here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "\n",
    "We start by carrying out the PCA of the toy data\n",
    "```Python\n",
    "pca = PCA()\n",
    "pca.fit(data);\n",
    "```\n",
    "\n",
    "Now, we plot a scatterplot of data, with arrows representing the first components of both tICA (red) and PCA (blue).\n",
    "\n",
    "```Python\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.scatter(data[:,0], data[:,1], marker = '.', color='black', alpha=0.1)\n",
    "\n",
    "origin = np.mean(data, axis=0)\n",
    "draw_arrow(origin, tic.eigenvectors_[0]*2, \"red\")\n",
    "draw_arrow(origin, pca.components_[0]*2, \"dodgerblue\")\n",
    "\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\");\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Comparison of dimensionality reduction with PCA and tICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction techniques enable us to identify suitable ways of projecting high-dimensional data into a lower-dimensional space with minimal information loss. We have just seen that PCA and tICA identify different spaces onto which the data can be projected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 5: </b>Project the data into the eigenspace generated by PCA, and into the tICA space. Create histograms of each of the components. What do you observe? </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Your solution here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "\n",
    "Let's project the data into the tICA and PCA spaces.\n",
    "    \n",
    "```Python    \n",
    "tic_out = tic.transform([data])[0]\n",
    "PCA_out = pca.transform(data)\n",
    "```\n",
    "<br>\n",
    "Now, let's make some pretty plots showing the projections on the first and second component of PCA and tICA.\n",
    "\n",
    "```Python\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "    \n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.hist(tic_out[:, 0], histtype=\"step\", label=\"tICA\", bins=50, color=\"red\")\n",
    "ax1.hist(PCA_out[:, 0], histtype=\"step\", label=\"PCA\", bins=50, color=\"dodgerblue\")\n",
    "ax1.set_xlabel(\"first component\")\n",
    "ax1.set_ylabel(\"count (#)\")\n",
    "ax1.legend(frameon=False);\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.hist(tic_out[:, 1], histtype=\"step\", label=\"tICA\", bins=50, color=\"red\")\n",
    "ax2.hist(PCA_out[:, 1], histtype=\"step\", label=\"PCA\", bins=50, color=\"dodgerblue\")\n",
    "ax2.set_xlabel(\"second component\")\n",
    "ax2.legend(frameon=False);\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Key points:</b>\n",
    "\n",
    "- PCA is a linear dimensionality reduction technique for tabular data,\n",
    "- PCA can be used to remove noise from data,\n",
    "- tICA is also a linear dimensionality reduction technique, but it maximises the autocorrelation time rather than the variance    \n",
    "- tICA and PCA may be appropriate for different use cases: tICA will generally provide you with slow dynamics and PCA for maximising spacial variance. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dimensionality Reduction, part 2](2_DR_part2.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
